{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input one:  tensor([-0.6842, -1.3532,  2.3002], requires_grad=True)\n",
      "input two:  tensor([-0.4707, -0.0685,  0.3754], requires_grad=True)\n",
      "target:  tensor([-1.,  1.,  1.])\n",
      "output:  tensor(0.4282, grad_fn=<MeanBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\braxt\\OneDrive\\Desktop\\DL Project with Latest Repo\\Deep-Learning-Project\\comparing_loss_funcs.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m ranking_loss \u001b[39m=\u001b[39m my_margin_ranking_loss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m output \u001b[39m=\u001b[39m ranking_loss\u001b[39m.\u001b[39mforward(input_one, input_two, target)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m output\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minput one: \u001b[39m\u001b[39m'\u001b[39m, input_one)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minput two: \u001b[39m\u001b[39m'\u001b[39m, input_two)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "##ex code using marginrankingloss (pytorch) --> pairwise LF\n",
    "input_one = torch.randn(3, requires_grad=True)\n",
    "input_two = torch.randn(3, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "\n",
    "ranking_loss = nn.MarginRankingLoss()\n",
    "output = ranking_loss(input_one, input_two, target)\n",
    "output.backward()\n",
    "\n",
    "print('input one: ', input_one)\n",
    "print('input two: ', input_two)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "print(type(torch.dot(-target,input_one-input_two)))\n",
    "##def class based on marginrankingloss\n",
    "\n",
    "#note to self: according to documentation I found, marginrankingloss does \n",
    "#ouput a tensor\n",
    "class my_margin_ranking_loss():\n",
    "\n",
    "    def __init__(self, margin=0.0) -> None:\n",
    "        #put margin into form can be accessed by other methods\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, input1, input2, target):\n",
    "        #equation from marginrankingloss documentation\n",
    "        # return torch.max(torch.tensor([0]),-target*(input1-input2)+self.margin)\n",
    "        # return torch.max(torch.constant([0]),torch.dot(-target,input1-input2)+self.margin)\n",
    "        my_dist = torch.dot(-target,input1-input2)\n",
    "\n",
    "        #turn tensor into scalar\n",
    "        scalar_dist = my_dist.item()\n",
    "        return max(0,int(scalar_dist+self.margin))\n",
    "\n",
    "    def forward(self, input1, input2, target):\n",
    "        #equation from marginrankingloss documentation\n",
    "        # return torch.max(torch.tensor([0]),-target*(input1-input2)+self.margin)\n",
    "        # return torch.max(torch.constant([0]),torch.dot(-target,input1-input2)+self.margin)\n",
    "        my_dist = torch.dot(-target,input1-input2)\n",
    "\n",
    "        #turn tensor into scalar\n",
    "        scalar_dist = my_dist.item()\n",
    "        return max(0,int(scalar_dist+self.margin))\n",
    "\n",
    "# return torch.max(torch.zeros(input1.size()),-target*(input1-input2)+torch.full(self.margin))\n",
    "\n",
    "\n",
    "ranking_loss = my_margin_ranking_loss()\n",
    "output = ranking_loss.forward(input_one, input_two, target)\n",
    "output.backward()\n",
    "\n",
    "print('input one: ', input_one)\n",
    "print('input two: ', input_two)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_Loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\braxt\\OneDrive\\Desktop\\DL Project with Latest Repo\\Deep-Learning-Project\\comparing_loss_funcs.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMarginRankingLoss\u001b[39;00m(_Loss):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Creates a criterion that measures the loss given\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    inputs :math:`x1`, :math:`x2`, two 1D mini-batch or 0D `Tensors`,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    and a label 1D mini-batch or 0D `Tensor` :math:`y` (containing 1 or -1).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m        >>> output.backward()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W2sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     __constants__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmargin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreduction\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name '_Loss' is not defined"
     ]
    }
   ],
   "source": [
    "class MarginRankingLoss(_Loss):\n",
    "    r\"\"\"Creates a criterion that measures the loss given\n",
    "    inputs :math:`x1`, :math:`x2`, two 1D mini-batch or 0D `Tensors`,\n",
    "    and a label 1D mini-batch or 0D `Tensor` :math:`y` (containing 1 or -1).\n",
    "    If :math:`y = 1` then it assumed the first input should be ranked higher\n",
    "    (have a larger value) than the second input, and vice-versa for :math:`y = -1`.\n",
    "    The loss function for each pair of samples in the mini-batch is:\n",
    "    .. math::\n",
    "        \\text{loss}(x1, x2, y) = \\max(0, -y * (x1 - x2) + \\text{margin})\n",
    "    Args:\n",
    "        margin (float, optional): Has a default value of :math:`0`.\n",
    "        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
    "            the losses are averaged over each loss element in the batch. Note that for\n",
    "            some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
    "            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
    "            when :attr:`reduce` is ``False``. Default: ``True``\n",
    "        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
    "            losses are averaged or summed over observations for each minibatch depending\n",
    "            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
    "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
    "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
    "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
    "            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
    "    Shape:\n",
    "        - Input1: :math:`(N)` or :math:`()` where `N` is the batch size.\n",
    "        - Input2: :math:`(N)` or :math:`()`, same shape as the Input1.\n",
    "        - Target: :math:`(N)` or :math:`()`, same shape as the inputs.\n",
    "        - Output: scalar. If :attr:`reduction` is ``'none'`` and Input size is not :math:`()`, then :math:`(N)`.\n",
    "    Examples::\n",
    "        >>> loss = nn.MarginRankingLoss()\n",
    "        >>> input1 = torch.randn(3, requires_grad=True)\n",
    "        >>> input2 = torch.randn(3, requires_grad=True)\n",
    "        >>> target = torch.randn(3).sign()\n",
    "        >>> output = loss(input1, input2, target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "    __constants__ = ['margin', 'reduction']\n",
    "    margin: float\n",
    "\n",
    "    def __init__(self, margin: float = 0., size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(MarginRankingLoss, self).__init__(size_average, reduce, reduction)\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, input1: Tensor, input2: Tensor, target: Tensor) -> Tensor:\n",
    "        return F.margin_ranking_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input one:  tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]], dtype=torch.float16, requires_grad=True)\n",
      "input two:  tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]], dtype=torch.float16, requires_grad=True)\n",
      "target:  tensor([-0.3481])\n",
      "output:  tensor(0., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "##ex code using marginrankingloss (pytorch) --> pairwise LF\n",
    "input_one = torch.tensor([[1, 2],\n",
    "                [3, 4],\n",
    "                [5, 6]], dtype=torch.float16,requires_grad=True)\n",
    "input_two = torch.tensor([[1, 2],\n",
    "                [3, 4],\n",
    "                [5, 6]], dtype=torch.float16,requires_grad=True)\n",
    "target = torch.randn(1)\n",
    "\n",
    "ranking_loss = nn.MarginRankingLoss()\n",
    "output = ranking_loss(input_one, input_two, target)\n",
    "output.backward()\n",
    "\n",
    "print('input one: ', input_one)\n",
    "print('input two: ', input_two)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\braxt\\OneDrive\\Desktop\\DL Project with Latest Repo\\Deep-Learning-Project\\comparing_loss_funcs.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmax(\u001b[39m0\u001b[39m,\u001b[39m-\u001b[39mtarget\u001b[39m*\u001b[39m(input1\u001b[39m-\u001b[39minput2)\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ranking_loss \u001b[39m=\u001b[39m my_margin_ranking_loss()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m ranking_loss(input_one, input_two, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m output\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braxt/OneDrive/Desktop/DL%20Project%20with%20Latest%20Repo/Deep-Learning-Project/comparing_loss_funcs.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minput one: \u001b[39m\u001b[39m'\u001b[39m, input_one)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_one' is not defined"
     ]
    }
   ],
   "source": [
    "##def class based on marginrankingloss\n",
    "\n",
    "#note to self: according to documentation I found, marginrankingloss does \n",
    "#ouput a tensor\n",
    "class my_margin_ranking_loss():\n",
    "\n",
    "    def __init__(self, margin=0.0) -> None:\n",
    "        #put margin into form can be accessed by other methods\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, input1, input2, target):\n",
    "        #equation from marginrankingloss documentation\n",
    "        return torch.max(0,-target*(input1-input2)+self.margin)\n",
    "\n",
    "        \n",
    "ranking_loss = my_margin_ranking_loss()\n",
    "output = ranking_loss(input_one, input_two, target)\n",
    "output.backward()\n",
    "\n",
    "print('input one: ', input_one)\n",
    "print('input two: ', input_two)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input one:  tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]], dtype=torch.float16, requires_grad=True)\n",
      "input two:  tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]], dtype=torch.float16, requires_grad=True)\n",
      "target:  tensor([0.8661])\n",
      "output:  tensor(0., grad_fn=<MeanBackward0>)\n",
      "input one:  tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], shape=(3, 2), dtype=float16)\n",
      "input two:  tf.Tensor(\n",
      "[[5. 9.]\n",
      " [3. 6.]\n",
      " [1. 8.]], shape=(3, 2), dtype=float16)\n",
      "target:  tf.Tensor([ 0.7173 -1.75   -0.6577], shape=(3,), dtype=float16)\n",
      "output:  tf.Tensor([ 46.62  -7.   -13.16], shape=(3,), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "##ex code using marginrankingloss (pytorch) --> pairwise LF\n",
    "input_one = torch.tensor([[1, 2],\n",
    "                [3, 4],\n",
    "                [5, 6]], dtype=torch.float16,requires_grad=True)\n",
    "input_two = torch.tensor([[1, 2],\n",
    "                [3, 4],\n",
    "                [5, 6]], dtype=torch.float16,requires_grad=True)\n",
    "target = torch.randn(1)\n",
    "\n",
    "ranking_loss = nn.MarginRankingLoss()\n",
    "output = ranking_loss(input_one, input_two, target)\n",
    "output.backward()\n",
    "\n",
    "print('input one: ', input_one)\n",
    "print('input two: ', input_two)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "##ex code using contrastive loss (tf)\n",
    "a = tf.constant([[1, 2],\n",
    "                [3, 4],\n",
    "                [5, 6]], dtype=tf.float16)\n",
    "b = tf.constant([[5, 9],\n",
    "                [3, 6],\n",
    "                [1, 8]], dtype=tf.float16)\n",
    "y_pred = tf.linalg.norm(a - b, axis=1)\n",
    "\n",
    "target = tf.random.normal([3],dtype=tf.float16)\n",
    "\n",
    "print('input one: ', a)\n",
    "print('input two: ', b)\n",
    "print('target: ', target)\n",
    "\n",
    "output = tfa.losses.contrastive_loss(target, y_pred, target)\n",
    "\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define lossI constructed in tensorflowd\n",
    "def loss(margin):\n",
    "    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        'constrastive_loss' function with data ('margin') attached.\n",
    "    \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(x1, x2,y):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing constrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "        #is this supposed to be a dot so that margin can be a scalar?\n",
    "        #from trial-and-error, it appears that you need to take the mean\n",
    "        # of these values.... ohhhhh, is 20 the batch size??\n",
    "        my_loss = tf.math.reduce_mean(tf.maximum(0,-y*(x1-x2)+margin))\n",
    "        return my_loss\n",
    "\n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "input one:  tensor([[1., 2.]], dtype=torch.float16, requires_grad=True)\n",
      "input two:  tensor([[5., 9.]], dtype=torch.float16, requires_grad=True)\n",
      "target:  tensor([[1., 1.]])\n",
      "output:  tensor(5.5000, grad_fn=<MeanBackward0>)\n",
      "input one:  tf.Tensor([[1. 2.]], shape=(1, 2), dtype=float16)\n",
      "input two:  tf.Tensor([[5. 9.]], shape=(1, 2), dtype=float16)\n",
      "target:  tf.Tensor([[1. 1.]], shape=(1, 2), dtype=float16)\n",
      "output:  tf.Tensor(5.5, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "##ex code using marginrankingloss (pytorch) --> pairwise LF\n",
    "input_one = torch.tensor([[1, 2]], dtype=torch.float16,requires_grad=True)\n",
    "input_two = torch.tensor([[5,9]], dtype=torch.float16,requires_grad=True)\n",
    "print(input_one.size())\n",
    "#create a target constructed in same manner as was done in implemented\n",
    "#paper \n",
    "target = torch.ones(input_one.size())\n",
    "\n",
    "ranking_loss = nn.MarginRankingLoss()\n",
    "output = ranking_loss(input_one, input_two, target)\n",
    "output.backward()\n",
    "\n",
    "print('input one: ', input_one)\n",
    "print('input two: ', input_two)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "##ex code using my version of margin ranking loss (tf)\n",
    "a = tf.constant([[1, 2]], dtype=tf.float16)\n",
    "b = tf.constant([[5, 9]], dtype=tf.float16)\n",
    "\n",
    "#create a target constructed in same manner as was done in implemented\n",
    "#paper \n",
    "target = tf.ones(tf.shape(a),dtype=tf.float16)\n",
    "\n",
    "print('input one: ', a)\n",
    "print('input two: ', b)\n",
    "print('target: ', target)\n",
    "\n",
    "#define instance of the loss I created\n",
    "#set margin = 0 bc that's what is set by default for MarginRankingLoss\n",
    "my_loss = loss(0)\n",
    "\n",
    "#set inputs to be same as inputs to MarginRankingLoss\n",
    "output = my_loss(a, b, target)\n",
    "\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##margin ranking loss notes\n",
    "article that describes it: https://onlinelibrary.wiley.com/doi/full/10.1002/cav.2036?casa_token=5j5gkFZ5Te4AAAAA%3ADCeRn-ImeC_IdPHnBafYDXoD9GK82y9zCQzaWAL9Unxdus6Sfygd12JYzHJw4XDlVZG1jaafu-_IfFI\n",
    "\n",
    "Three inputs the loss function according to the Paper we are implementing:\n",
    "- gold summary: should have the highest matching score\n",
    "- Candidate summary:one of two candidate summaries.I don't think we're supposed to know at this stage where the summary is being put into the loss function, what the better summary is. I think the loss function is somehow supposed to determinethat this is better period\n",
    "- Worse candidate summary: one of the two candidate summaries. I think, again, that the loss function is supposed to be what determines that this is the worst candidate summary.\n",
    "\n",
    "\n",
    "He is the score obtainedfrom the prediction vector. And the codes that we have, I thinkthis is from the variable \"score\". \n",
    "XI appears to be the index associated with the ground truth label ofthe document.\n",
    "\n",
    "\n",
    "The article below describes the positive sample as the gold summary and the negative sample as the silver summary.This, to me, suggests thatthe paper which we are examiningwould follow a similar protocol. \n",
    "https://arxiv.org/pdf/2108.11846.pdf\n",
    "\n",
    "I therefore revise my proposition of the inputs to the loss function to be the following: \n",
    "- gold summary (positive sample): should have the highest matching score\n",
    "- Candidate summary (negative sample):one of two candidate summaries.\n",
    "\n",
    "Quote from paper:\n",
    "\"However, there might exist a low-quality candidate that gets a higher score S. It is termed as\n",
    "“silver summary” when it is picked up as the output\n",
    "summary. The silver summary can be attributed to\n",
    "the discrepancy problem since the seq2seq model is\n",
    "only able to observe the gold summary at the time\n",
    "of training while the model needs to assess a large\n",
    "number of unseen alternatives at the time of inference. This problem is well-known as “exposure\n",
    "bias”.\"\n",
    "\n",
    "The paper suggests that the goal of usingcontrastive loss in the form of margin ranking loss is to explicitly decrease the score S of the silver summary during training.\n",
    "\n",
    "They seemed to argue that margin ranking lossworks to ensure that the positive score, IE, the score for the golden summary, is above that of the negative score, IE, the candidate summary.\n",
    "\n",
    "Loss = max(0,S(y_hat|X)-S(Y|X)+gamma)\n",
    "where gamma = margin value (distance apart we want the gold summary to be from the candidate summary)\n",
    "S(y_hat|X) = score from golden summary\n",
    "S(y|X) = silver summary\n",
    "\n",
    "Quote below is from the link below:\n",
    "https://arxiv.org/pdf/2104.09061.pdf\n",
    "\" We add a margin ranking loss term LRANK\n",
    "to encourage the model to assign higher probability\n",
    "to the positive than the negative candidate.\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5aa53336a2b522ef795d0a0092d3c472e84d5f553c71ebbb8b10824290e20ff7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
